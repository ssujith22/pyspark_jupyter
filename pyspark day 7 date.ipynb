{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "43f31bfd-8d59-4615-8131-91a084a566b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "current_date\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ed9ab7c-0461-49f7-9181-a45f29632e0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n|current_date|\n+------------+\n|  2024-10-30|\n+------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import current_date\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Current Date Example\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Get current date\n",
    "current_date_df = spark.range(1).select(current_date().alias(\"current_date\"))\n",
    "\n",
    "# Show the current date\n",
    "current_date_df.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d7ba4bb-86eb-4f30-afe9-a5f081217542",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+\n|date_string|      date|\n+-----------+----------+\n| 2024-04-18|2024-04-18|\n| 2023-12-25|2023-12-25|\n| 2022-09-10|2022-09-10|\n+-----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import to_date\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"to_date Example\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Sample DataFrame with date strings\n",
    "data = [(\"2024-04-18\",),\n",
    "        (\"2023-12-25\",),\n",
    "        (\"2022-09-10\",)]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"date_string\"])\n",
    "\n",
    "# Apply to_date function to convert string to date\n",
    "df = df.withColumn(\"date\", to_date(df[\"date_string\"]))\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "386cceba-c8a1-4e1b-aed0-93db836be6fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#In PySpark, the date_format() function is used to format a date or timestamp column according to a specified format string. Here's how you can use it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4d3a34b-93cb-446d-b5cd-7d3b095e93b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+--------------+\n|date_string|      date|formatted_date|\n+-----------+----------+--------------+\n| 2024-04-18|2024-04-18|    2024/04/18|\n| 2023-12-25|2023-12-25|    2023/12/25|\n| 2022-09-10|2022-09-10|    2022/09/10|\n+-----------+----------+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import date_format\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"date_format Example\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Sample DataFrame with a date column\n",
    "data = [(\"2024-04-18\",),\n",
    "        (\"2023-12-25\",),\n",
    "        (\"2022-09-10\",)]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"date_string\"])\n",
    "\n",
    "# Convert the date string column to a date type\n",
    "df = df.withColumn(\"date\", to_date(df[\"date_string\"]))\n",
    "\n",
    "# Apply date_format to format the date\n",
    "df = df.withColumn(\"formatted_date\", date_format(df[\"date\"], \"yyyy/MM/dd\"))\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "251b4865-b2d8-4dc3-8e0d-91fb91cf4a5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "the datediff() function is used to compute the difference between two dates. It returns the number of days between two dates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ca40232-07aa-4850-baba-83fc4dc0e8eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+----------+----------+---------------+\n|date1_string|date2_string|     date1|     date2|date_difference|\n+------------+------------+----------+----------+---------------+\n|  2024-04-18|  2024-04-10|2024-04-18|2024-04-10|              8|\n|  2023-12-25|  2023-11-25|2023-12-25|2023-11-25|             30|\n|  2022-09-10|  2022-09-01|2022-09-10|2022-09-01|              9|\n+------------+------------+----------+----------+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import datediff, to_date\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"datediff Example\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Sample DataFrame with two date columns\n",
    "data = [(\"2024-04-18\", \"2024-04-10\"),\n",
    "        (\"2023-12-25\", \"2023-11-25\"),\n",
    "        (\"2022-09-10\", \"2022-09-01\")]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"date1_string\", \"date2_string\"])\n",
    "\n",
    "# Convert the date string columns to date type\n",
    "df = df.withColumn(\"date1\", to_date(df[\"date1_string\"]))\n",
    "df = df.withColumn(\"date2\", to_date(df[\"date2_string\"]))\n",
    "\n",
    "# Apply datediff to calculate the difference in days\n",
    "df = df.withColumn(\"date_difference\", datediff(df[\"date1\"], df[\"date2\"]))\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cc364f7b-988f-4388-b58d-e718ee87e65c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "months_between() in pyspark\n",
    "the months_between() function is used to calculate the difference in months between two dates. It returns a float value representing the number of months between the two dates. Optionally, you can specify whether to include the fractional part in the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9766697-f17b-4de4-bfd9-2ffcf122b3ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+----------+----------+-----------------+\n|date1_string|date2_string|     date1|     date2|months_difference|\n+------------+------------+----------+----------+-----------------+\n|  2024-04-18|  2024-01-10|2024-04-18|2024-01-10|       3.25806452|\n|  2023-12-25|  2023-11-25|2023-12-25|2023-11-25|              1.0|\n|  2022-09-10|  2022-08-01|2022-09-10|2022-08-01|       1.29032258|\n+------------+------------+----------+----------+-----------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import months_between, to_date\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"months_between Example\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Sample DataFrame with two date columns\n",
    "data = [(\"2024-04-18\", \"2024-01-10\"),\n",
    "        (\"2023-12-25\", \"2023-11-25\"),\n",
    "        (\"2022-09-10\", \"2022-08-01\")]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"date1_string\", \"date2_string\"])\n",
    "\n",
    "# Convert the date string columns to date type\n",
    "df = df.withColumn(\"date1\", to_date(df[\"date1_string\"]))\n",
    "df = df.withColumn(\"date2\", to_date(df[\"date2_string\"]))\n",
    "\n",
    "# Apply months_between to calculate the difference in months\n",
    "df = df.withColumn(\"months_difference\", months_between(df[\"date1\"], df[\"date2\"]))\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "838be2a7-7a11-4676-86b4-30feeea88869",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "date_add() in pyspark\n",
    "\n",
    "the date_add() function is used to add or subtract a specified number of days to a date column. It returns a new date by adding the specified number of days to the input date. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76d16b6f-0170-4d46-a381-d442c71336f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+----------+\n|date_string|      date|date_added|\n+-----------+----------+----------+\n| 2024-04-18|2024-04-18|2024-04-23|\n| 2023-12-25|2023-12-25|2023-12-30|\n| 2022-09-10|2022-09-10|2022-09-15|\n+-----------+----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import date_add, to_date\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"date_add Example\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Sample DataFrame with a date column\n",
    "data = [(\"2024-04-18\",),\n",
    "        (\"2023-12-25\",),\n",
    "        (\"2022-09-10\",)]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"date_string\"])\n",
    "\n",
    "# Convert the date string column to a date type\n",
    "df = df.withColumn(\"date\", to_date(df[\"date_string\"]))\n",
    "\n",
    "# Apply date_add to add 5 days to the date\n",
    "df = df.withColumn(\"date_added\", date_add(df[\"date\"], 5))\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eaa2f6f8-1fa6-4a8b-8fa5-9ee830088f42",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "the month() function is used to extract the month component from a date or timestamp column. It returns an integer representing the month (1 for January, 2 for February, and so on). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c5cd724-b9ab-4a9d-9a56-650336e0bab5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+-----+\n|date_string|      date|month|\n+-----------+----------+-----+\n| 2024-04-18|2024-04-18|    4|\n| 2023-12-25|2023-12-25|   12|\n| 2022-09-10|2022-09-10|    9|\n+-----------+----------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import month, to_date\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"month Example\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Sample DataFrame with a date column\n",
    "data = [(\"2024-04-18\",),\n",
    "        (\"2023-12-25\",),\n",
    "        (\"2022-09-10\",)]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"date_string\"])\n",
    "\n",
    "# Convert the date string column to a date type\n",
    "df = df.withColumn(\"date\", to_date(df[\"date_string\"]))\n",
    "\n",
    "# Apply month function to extract the month\n",
    "df = df.withColumn(\"month\", month(df[\"date\"]))\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "318496ba-23eb-49f4-97d2-8e8da8a1d576",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "year() in pyspark\n",
    "\n",
    "the year() function is used to extract the year component from a date or timestamp column. It returns an integer representing the year. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73f09422-cc47-43a2-9209-d8d0b47b5677",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+----+\n|date_string|      date|year|\n+-----------+----------+----+\n| 2024-04-18|2024-04-18|2024|\n| 2023-12-25|2023-12-25|2023|\n| 2022-09-10|2022-09-10|2022|\n+-----------+----------+----+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import year, to_date\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"year Example\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Sample DataFrame with a date column\n",
    "data = [(\"2024-04-18\",),\n",
    "        (\"2023-12-25\",),\n",
    "        (\"2022-09-10\",)]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"date_string\"])\n",
    "\n",
    "# Convert the date string column to a date type\n",
    "df = df.withColumn(\"date\", to_date(df[\"date_string\"]))\n",
    "\n",
    "# Apply year function to extract the year\n",
    "df = df.withColumn(\"year\", year(df[\"date\"]))\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f31f3d6e-6e77-485b-9d9c-da1bd71ec05b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "the current_timestamp() function is used to retrieve the current timestamp. It returns the current timestamp as a timestamp type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27d10736-8818-4c87-a188-871e930af006",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n|current_timestamp      |\n+-----------------------+\n|2024-10-30 10:11:00.222|\n+-----------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import current_timestamp\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"current_timestamp Example\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Create a DataFrame with current timestamp\n",
    "df = spark.range(1).select(current_timestamp().alias(\"current_timestamp\"))\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show(truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7b7a9deb-d560-462c-8800-caa3de0cd5a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The to_timestamp() function is used to convert a string column or an expression representing a timestamp string into a timestamp type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e859f9f1-daf7-41fd-b58b-5a7a0e334250",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+\n|timestamp_string   |timestamp          |\n+-------------------+-------------------+\n|2024-04-18 12:34:56|2024-04-18 12:34:56|\n|2023-12-25 08:30:45|2023-12-25 08:30:45|\n|2022-09-10 15:20:10|2022-09-10 15:20:10|\n+-------------------+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"to_timestamp Example\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Sample DataFrame with timestamp strings\n",
    "data = [(\"2024-04-18 12:34:56\",),\n",
    "        (\"2023-12-25 08:30:45\",),\n",
    "        (\"2022-09-10 15:20:10\",)]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"timestamp_string\"])\n",
    "\n",
    "# Apply to_timestamp function to convert string to timestamp\n",
    "df = df.withColumn(\"timestamp\", to_timestamp(df[\"timestamp_string\"]))\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show(truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7028ee10-55ec-4fa3-8755-217772b4cd50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "In PySpark, the hour() function is used to extract the hour component from a timestamp column. It returns an integer representing the hour of the day (0 to 23). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03937ca0-cb50-483d-afce-a84930241021",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+----+\n|timestamp_string   |timestamp          |hour|\n+-------------------+-------------------+----+\n|2024-04-18 12:34:56|2024-04-18 12:34:56|12  |\n|2023-12-25 08:30:45|2023-12-25 08:30:45|8   |\n|2022-09-10 15:20:10|2022-09-10 15:20:10|15  |\n+-------------------+-------------------+----+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import hour, to_timestamp\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"hour Example\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Sample DataFrame with a timestamp column\n",
    "data = [(\"2024-04-18 12:34:56\",),\n",
    "        (\"2023-12-25 08:30:45\",),\n",
    "        (\"2022-09-10 15:20:10\",)]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"timestamp_string\"])\n",
    "\n",
    "# Convert the timestamp string column to a timestamp type\n",
    "df = df.withColumn(\"timestamp\", to_timestamp(df[\"timestamp_string\"]))\n",
    "\n",
    "# Apply hour function to extract the hour\n",
    "df = df.withColumn(\"hour\", hour(df[\"timestamp\"]))\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e150d75e-f20a-490e-8e75-0a65d7634044",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "In PySpark, the minute() function is used to extract the minute component from a timestamp column. It returns an integer representing the minute of the hour (0 to 59). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e54b37bf-ba4c-4352-ac6d-3bd9c265b2e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+------+\n|timestamp_string   |timestamp          |minute|\n+-------------------+-------------------+------+\n|2024-04-18 12:34:56|2024-04-18 12:34:56|34    |\n|2023-12-25 08:30:45|2023-12-25 08:30:45|30    |\n|2022-09-10 15:20:10|2022-09-10 15:20:10|20    |\n+-------------------+-------------------+------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import minute, to_timestamp\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"minute Example\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Sample DataFrame with a timestamp column\n",
    "data = [(\"2024-04-18 12:34:56\",),\n",
    "        (\"2023-12-25 08:30:45\",),\n",
    "        (\"2022-09-10 15:20:10\",)]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"timestamp_string\"])\n",
    "\n",
    "# Convert the timestamp string column to a timestamp type\n",
    "df = df.withColumn(\"timestamp\", to_timestamp(df[\"timestamp_string\"]))\n",
    "\n",
    "# Apply minute function to extract the minute\n",
    "df = df.withColumn(\"minute\", minute(df[\"timestamp\"]))\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show(truncate=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c5dd7257-81fc-4db8-b7e0-2b252719aef8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "In PySpark, there isn't a built-in function called seconds() to directly extract the seconds component from a timestamp column. However, you can achieve this by using a combination of other functions.\n",
    "\n",
    "You can use the second() function along with the to_timestamp() function to extract the seconds component from a timestamp column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a45d055a-7526-43e0-8a65-1826e9c472cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+-------+\n|timestamp_string   |timestamp          |seconds|\n+-------------------+-------------------+-------+\n|2024-04-18 12:34:56|2024-04-18 12:34:56|56     |\n|2023-12-25 08:30:45|2023-12-25 08:30:45|45     |\n|2022-09-10 15:20:10|2022-09-10 15:20:10|10     |\n+-------------------+-------------------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import second, to_timestamp\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"seconds Example\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Sample DataFrame with a timestamp column\n",
    "data = [(\"2024-04-18 12:34:56\",),\n",
    "        (\"2023-12-25 08:30:45\",),\n",
    "        (\"2022-09-10 15:20:10\",)]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"timestamp_string\"])\n",
    "\n",
    "# Convert the timestamp string column to a timestamp type\n",
    "df = df.withColumn(\"timestamp\", to_timestamp(df[\"timestamp_string\"]))\n",
    "\n",
    "# Apply second function to extract the seconds\n",
    "df = df.withColumn(\"seconds\", second(df[\"timestamp\"]))\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show(truncate=False)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "pyspark day 7 date",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
