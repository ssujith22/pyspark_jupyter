{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "19d5ed49-c477-4373-ba70-8722f5776f81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#1.Calculate the total purchase amount for each customer:\n",
    "\n",
    "\n",
    "data = [\n",
    "    (1, 100, \"2023-01-15\"),\n",
    "    (2, 150, \"2023-02-20\"),\n",
    "    (1, 200, \"2023-03-10\"),\n",
    "    (3, 50, \"2023-04-05\"),\n",
    "    (2, 120, \"2023-05-15\"),\n",
    "    (1, 300, \"2023-06-25\")\n",
    "]\n",
    "columns = [\"customer_id\", \"purchase_amount\", \"purchase_date\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1619584-0889-4922-8638-2c4ff5eb1d59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+-------------+\n|customer_id|purchase_amount|purchase_date|\n+-----------+---------------+-------------+\n|          1|            100|   2023-01-15|\n|          2|            150|   2023-02-20|\n|          1|            200|   2023-03-10|\n|          3|             50|   2023-04-05|\n|          2|            120|   2023-05-15|\n|          1|            300|   2023-06-25|\n+-----------+---------------+-------------+\n\n+-----------+---------------------+\n|customer_id|total_purchase_amount|\n+-----------+---------------------+\n|          1|                  600|\n|          2|                  270|\n|          3|                   50|\n+-----------+---------------------+\n\n"
     ]
    }
   ],
   "source": [
    "#Calculate the total purchase amount for each customer:\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "spark = SparkSession.builder.appName(\"CustomerPurchaseAnalysis\").getOrCreate()\n",
    "data = [\n",
    "    (1, 100, \"2023-01-15\"),\n",
    "    (2, 150, \"2023-02-20\"),\n",
    "    (1, 200, \"2023-03-10\"),\n",
    "    (3, 50, \"2023-04-05\"),\n",
    "    (2, 120, \"2023-05-15\"),\n",
    "    (1, 300, \"2023-06-25\")\n",
    "]\n",
    "columns = [\"customer_id\", \"purchase_amount\", \"purchase_date\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()\n",
    "total_purchase_per_customer = df.groupBy(\"customer_id\").agg(F.sum(\"purchase_amount\").alias(\"total_purchase_amount\"))\n",
    "total_purchase_per_customer.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0492e116-4ca1-455b-b672-e8dc5d6b7386",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#2.Find the customer with the highest total purchase amount\n",
    "\n",
    "data = [\n",
    "    (1, 100, \"2023-01-15\"),\n",
    "    (2, 150, \"2023-02-20\"),\n",
    "    (1, 200, \"2023-03-10\"),\n",
    "    (3, 50, \"2023-04-05\"),\n",
    "    (2, 120, \"2023-05-15\"),\n",
    "    (1, 300, \"2023-06-25\")\n",
    "]\n",
    "columns = [\"customer_id\", \"purchase_amount\", \"purchase_date\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea42c517-eb3d-40f3-9312-34476f877d7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------------+\n|customer_id|total_purchase_amount|\n+-----------+---------------------+\n|          1|                  600|\n|          2|                  270|\n|          3|                   50|\n+-----------+---------------------+\n\nCustomer with highest purchase: 1\n"
     ]
    }
   ],
   "source": [
    "#2.Find the customer with the highest total purchase amount\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "spark = SparkSession.builder.appName(\"CustomerPurchaseAnalysis\").getOrCreate()\n",
    "data = [\n",
    "    (1, 100, \"2023-01-15\"),\n",
    "    (2, 150, \"2023-02-20\"),\n",
    "    (1, 200, \"2023-03-10\"),\n",
    "    (3, 50, \"2023-04-05\"),\n",
    "    (2, 120, \"2023-05-15\"),\n",
    "    (1, 300, \"2023-06-25\")\n",
    "]\n",
    "columns = [\"customer_id\", \"purchase_amount\", \"purchase_date\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "total_purchase_per_customer = df.groupBy(\"customer_id\").agg(F.sum(\"purchase_amount\").alias(\"total_purchase_amount\"))\n",
    "customer_with_highest_purchase = total_purchase_per_customer.orderBy(F.desc(\"total_purchase_amount\")).first()\n",
    "#df.show()\n",
    "total_purchase_per_customer.show()\n",
    "print(\"Customer with highest purchase:\", customer_with_highest_purchase[\"customer_id\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "307f71cc-3166-45b4-8890-94d24f042d48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "Analyzing Employee Salaries\n",
    "\n",
    "Task: You have a dataset containing information about employee salaries in a company. Your task is to use PySpark to analyze the data and answer a few questions using aggregate functions.\n",
    "\n",
    "Dataset: The dataset is in CSV format and contains the following columns: employee_id, employee_name, department, salary.\n",
    "\n",
    "employee_id,employee_name,department,salary\n",
    "1,John Doe,Engineering,90000\n",
    "2,Jane Smith,Marketing,75000\n",
    "3,Michael Johnson,Engineering,105000\n",
    "4,Emily Davis,Marketing,80000\n",
    "5,Robert Brown,Engineering,95000\n",
    "6,Linda Wilson,HR,60000\n",
    "\n",
    "Questions:\n",
    "\n",
    "Calculate the total payroll cost for the company.\n",
    "Find the average salary for each department.\n",
    "Identify the highest-paid employee and their department.\n",
    "Calculate the total number of employees in each department."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f98c2ef8-7329-432f-9f49-1ac295da8055",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Payroll Cost: 505000\n+-----------+-----------------+\n| department|       avg_salary|\n+-----------+-----------------+\n|Engineering|96666.66666666667|\n|  Marketing|          77500.0|\n|         HR|          60000.0|\n+-----------+-----------------+\n\nHighest-Paid Employee: Row(employee_name='Michael Johnson', department='Engineering', salary=105000)\n+-----------+---------------+\n| department|total_employees|\n+-----------+---------------+\n|Engineering|              3|\n|  Marketing|              2|\n|         HR|              1|\n+-----------+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum, avg, max, count\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"EmployeeAnalysis\").getOrCreate()\n",
    "\n",
    "# Load the dataset\n",
    "data = [\n",
    "    (1, \"John Doe\", \"Engineering\", 90000),\n",
    "    (2, \"Jane Smith\", \"Marketing\", 75000),\n",
    "    (3, \"Michael Johnson\", \"Engineering\", 105000),\n",
    "    (4, \"Emily Davis\", \"Marketing\", 80000),\n",
    "    (5, \"Robert Brown\", \"Engineering\", 95000),\n",
    "    (6, \"Linda Wilson\", \"HR\", 60000)\n",
    "]\n",
    "columns = [\"employee_id\", \"employee_name\", \"department\", \"salary\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Question 1: Calculate total payroll cost\n",
    "total_payroll = df.select(sum(\"salary\")).collect()[0][0]\n",
    "print(\"Total Payroll Cost:\", total_payroll)\n",
    "\n",
    "# Question 2: Average salary per department\n",
    "avg_salary_per_department = df.groupBy(\"department\").agg(avg(\"salary\").alias(\"avg_salary\"))\n",
    "avg_salary_per_department.show()\n",
    "\n",
    "\n",
    "# Question 3: Highest-paid employee and their department\n",
    "highest_paid_employee = df.orderBy(col(\"salary\").desc()).limit(1).select(\"employee_name\", \"department\", \"salary\").first()\n",
    "print(\"Highest-Paid Employee:\", highest_paid_employee)\n",
    "\n",
    "# Question 4: Total number of employees per department\n",
    "total_employees_per_department = df.groupBy(\"department\").agg(count(\"employee_id\").alias(\"total_employees\"))\n",
    "total_employees_per_department.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3a71e034-a3b7-43f7-872c-476062a87673",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "PySpark Coding Challenge: Analyzing Online Store Orders\n",
    "\n",
    "Task: You have a dataset containing information about orders from an online store. Your task is to use PySpark to analyze the data and answer a few questions using aggregate functions.\n",
    "\n",
    "Dataset: The dataset is in CSV format and contains the following columns: order_id, customer_id, order_date, total_amount.\n",
    "\n",
    "data = [\n",
    "    (1, \"C101\", \"2023-07-01\", 150),\n",
    "    (2, \"C102\", \"2023-07-02\", 200),\n",
    "    (3, \"C101\", \"2023-07-02\", 100),\n",
    "    (4, \"C103\", \"2023-07-03\", 300),\n",
    "    (5, \"C102\", \"2023-07-04\", 250),\n",
    "    (6, \"C101\", \"2023-07-05\", 120)\n",
    "]\n",
    "\n",
    "Questions:\n",
    "\n",
    "Calculate the total revenue generated from all orders.\n",
    "Find the average order amount.\n",
    "Identify the highest total order amount and its corresponding customer.\n",
    "Calculate the total number of orders for each customer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ddbb534-6b9e-4622-a18c-cd75c91de4de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Revenue: 1120\nAverage Order Amount: 186.66666666666666\nHighest Order Amount: 300\nCustomer ID: C103\n+-----------+------------+\n|customer_id|total_orders|\n+-----------+------------+\n|       C101|           3|\n|       C102|           2|\n|       C103|           1|\n+-----------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum, avg, max, count\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"OnlineStoreAnalysis\").getOrCreate()\n",
    "\n",
    "# Load the dataset\n",
    "data = [\n",
    "    (1, \"C101\", \"2023-07-01\", 150),\n",
    "    (2, \"C102\", \"2023-07-02\", 200),\n",
    "    (3, \"C101\", \"2023-07-02\", 100),\n",
    "    (4, \"C103\", \"2023-07-03\", 300),\n",
    "    (5, \"C102\", \"2023-07-04\", 250),\n",
    "    (6, \"C101\", \"2023-07-05\", 120)\n",
    "]\n",
    "columns = [\"order_id\", \"customer_id\", \"order_date\", \"total_amount\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Question 1: Calculate total revenue\n",
    "total_revenue = df.select(sum(\"total_amount\")).collect()[0][0]\n",
    "print(\"Total Revenue:\", total_revenue)\n",
    "\n",
    "# Question 2: Average order amount\n",
    "average_order_amount = df.agg(avg(\"total_amount\")).collect()[0][0]\n",
    "print(\"Average Order Amount:\", average_order_amount)\n",
    "\n",
    "# Question 3: Highest total order amount and corresponding customer\n",
    "highest_order = df.orderBy(col(\"total_amount\").desc()).limit(1).first()\n",
    "print(\"Highest Order Amount:\", highest_order[\"total_amount\"])\n",
    "print(\"Customer ID:\", highest_order[\"customer_id\"])\n",
    "\n",
    "# Question 4: Total number of orders per customer\n",
    "total_orders_per_customer = df.groupBy(\"customer_id\").agg(count(\"order_id\").alias(\"total_orders\"))\n",
    "total_orders_per_customer.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9c911f45-6f30-4bce-9b3a-0782b70c2cf6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Question 1: Average score per subject\n",
    "# Question 2: Highest score and corresponding student per subject\n",
    "# Question 3: Total number of students per subject\n",
    "# Question 4: Subject(s) with the highest average score\n",
    "\n",
    "data = [\n",
    "    (1, \"Math\", 85),\n",
    "    (2, \"Science\", 92),\n",
    "    (3, \"Math\", 78),\n",
    "    (4, \"English\", 88),\n",
    "    (5, \"Science\", 95),\n",
    "    (6, \"Math\", 90)\n",
    "]\n",
    "columns = [\"student_id\", \"subject\", \"score\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f413d350-0571-46c1-b93a-9a839c61a4ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, avg, max, count\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"StudentScoresAnalysis\").getOrCreate()\n",
    "\n",
    "# Load the dataset\n",
    "data = [\n",
    "    (1, \"Math\", 85),\n",
    "    (2, \"Science\", 92),\n",
    "    (3, \"Math\", 78),\n",
    "    (4, \"English\", 88),\n",
    "    (5, \"Science\", 95),\n",
    "    (6, \"Math\", 90)\n",
    "]\n",
    "columns = [\"student_id\", \"subject\", \"score\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Question 1: Average score per subject\n",
    "avg_score_per_subject = df.groupBy(\"subject\").agg(avg(\"score\").alias(\"avg_score\"))\n",
    "avg_score_per_subject.show()\n",
    "\n",
    "# Question 2: Highest score and corresponding student per subject\n",
    "highest_score_per_subject = df.groupBy(\"subject\").agg(max(\"score\").alias(\"highest_score\"))\n",
    "highest_score_students = df.join(highest_score_per_subject, on=\"subject\").filter(col(\"score\") == col(\"highest_score\"))\n",
    "highest_score_students.show()\n",
    "\n",
    "# Question 3: Total number of students per subject\n",
    "total_students_per_subject = df.groupBy(\"subject\").agg(count(\"student_id\").alias(\"total_students\"))\n",
    "total_students_per_subject.show()\n",
    "\n",
    "# Question 4: Subject(s) with the highest average score\n",
    "highest_avg_score_subject = avg_score_per_subject.orderBy(col(\"avg_score\").desc()).limit(1).first()\n",
    "print(\"Subject with Highest Average Score:\", highest_avg_score_subject[\"subject\"])"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Day 8",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
